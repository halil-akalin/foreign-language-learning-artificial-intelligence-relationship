setwd("D:/Code/R/socialMediaWebAnalysis/dataProcessingPhase")
getwd()
file_path <- "cleanedStopWordsAndConjunctions.txt"
text_content <- readLines(file_path)
#<------------------------------------------------------------------------------------------------------>
# Tokenize islemleri
library(tokenizers)
# Tokenize etmek icin fonksiyonumuzu yazalim
tokenized_text <- function(text) {
tokens <- unlist(tokenize_words(text))
return(tokens)
}
tokens <- lapply(text_content, tokenized_text)
print(tokens)
# Metni tokenize ediyorum
all_tokens <- unlist(tokens)
# Kelime frekansi adimi
word_freq <- table(all_tokens)
# Kelimeleri buyukten kucuge siraliyorum
sorted_word_freq <- sort(word_freq, decreasing = TRUE)
head(sorted_word_freq, 10)
#<------------------------------------------------------------------------------------------------------>
# Data gorsellestirme adimi
library(wordcloud)
# Tokenlari birlestirme adimi
all_tokens <- unlist(tokens)
# Kelime frekansini hesaplama adimi
word_freq <- table(all_tokens)
# Kelime sikligini en buyukten en kucuge siralama
sorted_word_freq <- sort(word_freq, decreasing = TRUE)
# Word cloud olusturma
wordcloud(words = names(sorted_word_freq), freq = sorted_word_freq, scale=c(3,0.5), min.freq = 15, colors=brewer.pal(8, "Dark2"))
#<------------------------------------------------------------------------------------------------------>
# Tokenize edilmis veri ile cubuk grafigi olusturma
library(ggplot2)
print(sorted_word_freq)
# Veri kumesinde en cok kullanilan 20 kelimeden olusan bir veri cercevesi olusturma adimi
top_words <- head(sorted_word_freq, 20)
df <- data.frame(word = names(top_words), Freq = as.numeric(top_words))
print(df)
# Tokenize edilmis veri ile cubuk grafik olusturma
ggplot(df, aes(x = Freq, y = reorder(word, Freq))) +
geom_bar(stat = "identity", fill = "lightgreen") +
ggtitle("En Cok Kullanilan Kelimelerin Frekanslari") +
ylab("Kelime") +
xlab("Frekans") +
theme(axis.text.y = element_text(hjust = 1))
#<------------------------------------------------------------------------------------------------------>
library(tidytext)
library(dplyr)
# NRC duygu sozlugunu kullanarak duygu analizi yapma adimi
nrc <- get_sentiments("nrc")
# Duygu analizi icin veri cercevesi olusturalim
data_tidy_nrc <- data.frame(word = all_tokens) %>%
inner_join(nrc, by = "word")
# Duygu skorlarini toplayalim
sentiment_scores_nrc <- data_tidy_nrc %>%
count(sentiment, sort = TRUE)
# Sonuclari gosterelim
print(sentiment_scores_nrc)
#<------------------------------------------------------------------------------------------------------>
library(ggplot2)
# Duygu skorlarinin toplamini hesaplayalm
total_n <- sum(sentiment_scores_nrc$n)
# YC<zdelik degerleri hesaplayalim
sentiment_scores_nrc <- sentiment_scores_nrc %>%
mutate(percent = n / total_n * 100)
# Pasta grafigi cizme adimi ve yuzdelik degerleri ekleme
ggplot(sentiment_scores_nrc, aes(x = "", y = n, fill = sentiment)) +
geom_bar(stat = "identity", width = 1) +
coord_polar("y", start = 0) +
theme_void() +
scale_fill_brewer(palette = "Set3") +
ggtitle("NRC Duygu Sozlugune Gore Duygu Dagilimi") +
geom_text(aes(label = sprintf("%.1f%%", percent)), position = position_stack(vjust = 0.5))
#<------------------------------------------------------------------------------------------------------>
# NRC duygu sozlugunu kullanarak duygu analizi
library(dplyr)
library(ggplot2)
library(tidytext)
# Her duygu kategorisi icin en fazla tekrar eden ilk iki kelimeyi sececegiz veeee
top_nrc_words <- data_tidy_nrc %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(1, n) %>%
ungroup()
# veeee Burada cubuk grafik cizecegiz
ggplot(top_nrc_words, aes(x = word, y = n, fill = sentiment)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
scale_fill_brewer(palette = "Set3") +
ggtitle("Her Duygu Kategorisi icin En Fazla Tekrar Eden ilk iki Kelime") +
xlab("Kelime") +
ylab("Frekans")
#<------------------------------------------------------------------------------------------------------>
# Oncelikle, duygu ve emoji eslesmelerimizi bir liste olarak tanimlayalim
library(tidyr)
sentiment_emoji <- list(
"positive" = c("ðŸ˜Š", "ðŸ˜", "ðŸ‘"),
"negative" = c("ðŸ˜¢", "ðŸ˜ž", "ðŸ‘Ž"),
"neutral" = c("ðŸ˜", "ðŸ¤”"),
"anger" = c("ðŸ˜¡", "ðŸ‘¿"),
"anticipation" = c("ðŸ˜¯", "ðŸ¤ž"),
"disgust" = c("ðŸ¤¢", "ðŸ¤®"),
"fear" = c("ðŸ˜¨", "ðŸ˜±"),
"joy" = c("ðŸ˜‚", "ðŸ¥³"),
"sadness" = c("ðŸ˜­", "ðŸ˜”"),
"surprise" = c("ðŸ˜²", "ðŸ˜³"),
"trust" = c("ðŸ¤", "ðŸ‘Œ")
)
# Duygu analizi icin veri cercevesi olusturalim
data_tidy_nrc <- data.frame(word = all_tokens) %>%
inner_join(nrc, by = "word")
# Her kelime icin duygu kategorisini atayalim
data_tidy_nrc <- data_tidy_nrc %>%
group_by(word) %>%
summarize(sentiment = toString(unique(sentiment)))
# Her kategori icin rastgele bir emoji secelim veee
assign_emoji <- function(sentiments) {
sentiments <- unlist(strsplit(sentiments, ", "))
emojis <- unlist(lapply(sentiments, function(s) sample(sentiment_emoji[[s]], size = 1)))
return(toString(emojis))
}
# veee her kelimeye rastgele bir emoji atayalim
data_tidy_nrc$emojis <- sapply(data_tidy_nrc$sentiment, assign_emoji)
# Sonuclari kontrol edelim (HATA ALDIGIM ICIN KONTROL AMACLI YAZILDI)
head(data_tidy_nrc)
# 'emojis' sutununun dogru formatta oldugunu kontrol edelim (HATA ALDIGIM ICIN KONTROL AMACLI YAZILDI)
data_tidy_nrc$emojis <- as.character(data_tidy_nrc$emojis)
# 'emojis' sutununda NA degerlerin olmadigini kontrol edelim (HATA ALDIGIM ICIN KONTROL AMACLI YAZILDI)
data_tidy_nrc <- data_tidy_nrc[!is.na(data_tidy_nrc$emojis), ]
# 'emoji sutununu bir liste olarak donusturelim ve unnest_tokens fonksiyonunu uygulayalim
data_tidy_nrc$emojis <- strsplit(data_tidy_nrc$emojis, ", ")
emoji_freq <- data_tidy_nrc %>%
unnest(emojis) %>%
group_by(emojis) %>%
summarise(Frequency = n()) %>%
ungroup()
# Emojileri duygu sozluguyle esledik. Ortaya cD1kan agirlik dagilimini pasta grafigi uzerinde gosterelim
library(ggplot2)
# Emoji frekanslarinin toplamini hesaplayalim
total_freq <- sum(emoji_freq$Frequency)
# Yuzdelik degerleri hesaplayalim
emoji_freq <- emoji_freq %>%
mutate(percent = Frequency / total_freq * 100)
# Pasta grafigi cizme adimi ve yuzdelik degerleri ekleme
ggplot(emoji_freq, aes(x = "", y = Frequency, fill = emojis)) +
geom_bar(stat = "identity", width = 1) +
coord_polar("y", start = 0) +
theme_void() +
theme(legend.position = "right") +
ggtitle("Emoji FrekanslarD1na Gore Pasta GrafiDi") +
geom_label(aes(label = sprintf("%.1f%%", percent)), position = position_stack(vjust = 0.5), size = 3 * 0.9)
#<------------------------------------------------------------------------------------------------------>
library(ggplot2)
library(dplyr)
# Satir basliklarini gormek icin kullaniyoruz
colnames(emoji_freq)
# emoji_freq veri cercevesinin ilk birkac satirini goruntuleme
print(head(emoji_freq))
# En cok tekrar eden 20 emojiyi secme
top_20_emojis <- emoji_freq %>%
arrange(desc(Frequency)) %>%
head(20)
# Secilen verileri goruntuleme
print(top_20_emojis)
# En fazla frekans sayisina sahip 20 emojinin yatay cubuk grafigi olusturalim
ggplot(top_20_emojis, aes(x = Frequency, y = reorder(emojis, Frequency))) +
geom_bar(stat = "identity", fill = "#FD6467") +
ggtitle("En Cok Kullanilan 20 Emojinin Frekanslari") +
ylab("Emoji") +
xlab("Frekans") +
theme(axis.text.y = element_text(hjust = 1))
#<------------------------------------------------------------------------------------------------------>
# Emoji bulutunu cizdirelim
library(wordcloud)
# Emoji frekanslarini kullanarak emoji bulutunu olusturalim
# Zaten emoji frekansini cubuk grafikte gosterdim bu yuzden emoji cloud cizdirmek biraz gereksiz bence,
# kodu uzatiyor ve analiz edilecek yeni bir gorsel materyal ortaya cikariyor.
wordcloud(words = emoji_freq$emojis, freq = emoji_freq$Frequency, scale=c(3,0.5), min.freq = 15, colors=brewer.pal(8, "Set2"))
#<------------------------------------------------------------------------------------------------------>
# Word cloud olusturma
wordcloud(words = names(sorted_word_freq), freq = sorted_word_freq, scale=c(3,0.5), min.freq = 15, colors=brewer.pal(8, "Dark2"))
# Word cloud olusturma
wordcloud(words = names(sorted_word_freq), freq = sorted_word_freq, scale=c(3,0.5), min.freq = 15, colors=brewer.pal(8, "Dark2"))
# Tokenize edilmis veri ile cubuk grafik olusturma
ggplot(df, aes(x = Freq, y = reorder(word, Freq))) +
geom_bar(stat = "identity", fill = "lightgreen") +
ggtitle("Frequencies of Most Used Words") +
ylab("Kelime") +
xlab("Frekans") +
theme(axis.text.y = element_text(hjust = 1))
# Pasta grafigi cizme adimi ve yuzdelik degerleri ekleme
ggplot(sentiment_scores_nrc, aes(x = "", y = n, fill = sentiment)) +
geom_bar(stat = "identity", width = 1) +
coord_polar("y", start = 0) +
theme_void() +
scale_fill_brewer(palette = "Set3") +
ggtitle("NRC Duygu Sozlugune Gore Duygu Dagilimi") +
geom_text(aes(label = sprintf("%.1f%%", percent)), position = position_stack(vjust = 0.5))
# Pasta grafigi cizme adimi ve yuzdelik degerleri ekleme
ggplot(sentiment_scores_nrc, aes(x = "", y = n, fill = sentiment)) +
geom_bar(stat = "identity", width = 1) +
coord_polar("y", start = 0) +
theme_void() +
scale_fill_brewer(palette = "Set3") +
ggtitle("Emotion Distribution According to NRC Emotion Dictionary") +
geom_text(aes(label = sprintf("%.1f%%", percent)), position = position_stack(vjust = 0.5))
# veeee Burada cubuk grafik cizecegiz
ggplot(top_nrc_words, aes(x = word, y = n, fill = sentiment)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
scale_fill_brewer(palette = "Set3") +
ggtitle("Her Duygu Kategorisi icin En Fazla Tekrar Eden ilk iki Kelime") +
xlab("Kelime") +
ylab("Frekans")
# veeee Burada cubuk grafik cizecegiz
ggplot(top_nrc_words, aes(x = word, y = n, fill = sentiment)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
scale_fill_brewer(palette = "Set3") +
ggtitle("The first two words that occur most frequently for each emotion category") +
xlab("Kelime") +
ylab("Frekans")
# Pasta grafigi cizme adimi ve yuzdelik degerleri ekleme
ggplot(emoji_freq, aes(x = "", y = Frequency, fill = emojis)) +
geom_bar(stat = "identity", width = 1) +
coord_polar("y", start = 0) +
theme_void() +
theme(legend.position = "right") +
ggtitle("Emoji FrekanslarD1na Gore Pasta GrafiDi") +
geom_label(aes(label = sprintf("%.1f%%", percent)), position = position_stack(vjust = 0.5), size = 3 * 0.9)
# Pasta grafigi cizme adimi ve yuzdelik degerleri ekleme
ggplot(emoji_freq, aes(x = "", y = Frequency, fill = emojis)) +
geom_bar(stat = "identity", width = 1) +
coord_polar("y", start = 0) +
theme_void() +
theme(legend.position = "right") +
ggtitle("Pie Chart According to Emoji Frequencies") +
geom_label(aes(label = sprintf("%.1f%%", percent)), position = position_stack(vjust = 0.5), size = 3 * 0.9)
# En fazla frekans sayisina sahip 20 emojinin yatay cubuk grafigi olusturalim
ggplot(top_20_emojis, aes(x = Frequency, y = reorder(emojis, Frequency))) +
geom_bar(stat = "identity", fill = "#FD6467") +
ggtitle("En Cok Kullanilan 20 Emojinin Frekanslari") +
ylab("Emoji") +
xlab("Frekans") +
theme(axis.text.y = element_text(hjust = 1))
# En fazla frekans sayisina sahip 20 emojinin yatay cubuk grafigi olusturalim
ggplot(top_20_emojis, aes(x = Frequency, y = reorder(emojis, Frequency))) +
geom_bar(stat = "identity", fill = "#FD6467") +
ggtitle("Frequencies of the 20 Most Used Emojis") +
ylab("Emoji") +
xlab("Frekans") +
theme(axis.text.y = element_text(hjust = 1))
# Emoji frekanslarini kullanarak emoji bulutunu olusturalim
# Zaten emoji frekansini cubuk grafikte gosterdim bu yuzden emoji cloud cizdirmek biraz gereksiz bence,
# kodu uzatiyor ve analiz edilecek yeni bir gorsel materyal ortaya cikariyor.
wordcloud(words = emoji_freq$emojis, freq = emoji_freq$Frequency, scale=c(3,0.5), min.freq = 15, colors=brewer.pal(8, "Set2"))
