source("D:/Code/R/socialMediaWebAnalysis/apiKey.R")
source("D:/Code/R/socialMediaWebAnalysis/apiKey.R")
# Videoların ID'lerini belirtin
video_ids <- c("CYwG8LmjmI8", "5Jwtv1rRa-A", "SupKb2CSrlE", "nGMYhO9OD9w&t=1s")
all_comments <- list()
for (video_id in video_ids) {
# API isteğini oluşturun
url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads",
"?part=snippet",
"&videoId=", video_id,
"&key=", api_key)
# API'den yorumları alın
response <- GET(url)
if (http_error(response)) {
stop("API isteği başarısız oldu. Hata kodu: ", http_status(response)$code)
}
# Yanıtı JSON formatına çevirin
comments_json <- content(response, "text")
comments <- fromJSON(comments_json)
# Sadece yorum metinlerini alın ve birleştirin
video_comments <- sapply(comments$items, function(item) item$snippet$topLevelComment$snippet$textDisplay)
# Tüm yorumları listeye ekleyin
all_comments[[video_id]] <- video_comments
}
library(httr)
library(jsonlite)
all_comments <- list()
for (video_id in video_ids) {
# API isteğini oluşturun
url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads",
"?part=snippet",
"&videoId=", video_id,
"&key=", api_key)
# API'den yorumları alın
response <- GET(url)
if (http_error(response)) {
stop("API isteği başarısız oldu. Hata kodu: ", http_status(response)$code)
}
# Yanıtı JSON formatına çevirin
comments_json <- content(response, "text")
comments <- fromJSON(comments_json)
# Sadece yorum metinlerini alın ve birleştirin
video_comments <- sapply(comments$items, function(item) item$snippet$topLevelComment$snippet$textDisplay)
# Tüm yorumları listeye ekleyin
all_comments[[video_id]] <- video_comments
}
comments
# Tüm yorumları tutacak bir liste oluşturalım
all_comments <- list()
# Her bir video için yorumları alalım
for (item in comments$items) {
# Yorumları içeren alanı seçelim
comment_text <- item$snippet$topLevelComment$snippet$textDisplay
# Yorumları listeye ekleyelim
all_comments <- c(all_comments, comment_text)
}
print(str(comments))
all_comments <- list()
for (item in comments$items) {
if (!is.null(item$snippet$topLevelComment$snippet$textDisplay)) {
comment_text <- item$snippet$topLevelComment$snippet$textDisplay
all_comments <- c(all_comments, comment_text)
}
}
all_comments <- list()
for (i in 1:nrow(comments$items)) {
comment_text <- comments$items$snippet$topLevelComment$snippet$textDisplay[i]
if (!is.null(comment_text)) {
all_comments <- c(all_comments, comment_text)
}
}
url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads",
"?part=snippet",
"&videoId=", video_id,
"&maxResults=50",  # 50 yorum çekmek için
"&key=", api_key)
url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads",
"?part=snippet",
"&videoId=", video_id,
"&maxResults=1500",  # 1500 yorum çekmek için
"&key=", api_key)
source("D:/Code/R/socialMediaWebAnalysis/apiKey.R")
library(httr)
library(jsonlite)
# Videoların ID'lerini belirtin
video_ids <- c("CYwG8LmjmI8", "5Jwtv1rRa-A", "SupKb2CSrlE", "nGMYhO9OD9w&t=1s")
url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads",
"?part=snippet",
"&videoId=", video_id,
"&maxResults=1500",  # 1500 yorum çekmek için
"&key=", api_key)
url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads",
"?part=snippet",
"&videoId=", video_ids,
"&maxResults=1500",  # 1500 yorum çekmek için
"&key=", api_key)
# API'den yorumları alın
response <- GET(url)
response <- GET(url)
# Başlangıç değerleri
all_comments <- list()
max_comments <- 1500
# Döngü
page_token <- ""
while (length(all_comments) < max_comments) {
# API isteğini oluşturun
url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads",
"?part=snippet",
"&videoId=", video_id,
"&maxResults=50",  # Bir istekte 50 yorum alın
"&key=", api_key,
"&pageToken=", page_token)
# API'den yorumları alın
response <- GET(url)
if (http_error(response)) {
stop("API isteği başarısız oldu. Hata kodu: ", http_status(response)$code)
}
# Yanıtı JSON formatına çevirin
comments_json <- content(response, "text")
comments <- fromJSON(comments_json)
# Sadece yorum metinlerini alın ve birleştirin
video_comments <- sapply(comments$items, function(item) item$snippet$topLevelComment$snippet$textDisplay)
# Tüm yorumları listeye ekleyin
all_comments <- c(all_comments, video_comments)
# Bir sonraki sayfa için sayfa belirteci alın
page_token <- comments$nextPageToken
# Sonraki sayfa yoksa döngüyü sonlandır
if (is.null(page_token)) {
break
}
}
# Döngü
page_token <- ""
while (length(all_comments) < max_comments) {
# API isteğini oluşturun
url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads",
"?part=snippet",
"&videoId=", video_ids,
"&maxResults=50",  # Bir istekte 50 yorum alın
"&key=", api_key,
"&pageToken=", page_token)
# API'den yorumları alın
response <- GET(url)
if (http_error(response)) {
stop("API isteği başarısız oldu. Hata kodu: ", http_status(response)$code)
}
# Yanıtı JSON formatına çevirin
comments_json <- content(response, "text")
comments <- fromJSON(comments_json)
# Sadece yorum metinlerini alın ve birleştirin
video_comments <- sapply(comments$items, function(item) item$snippet$topLevelComment$snippet$textDisplay)
# Tüm yorumları listeye ekleyin
all_comments <- c(all_comments, video_comments)
# Bir sonraki sayfa için sayfa belirteci alın
page_token <- comments$nextPageToken
# Sonraki sayfa yoksa döngüyü sonlandır
if (is.null(page_token)) {
break
}
}
source("D:/Code/R/socialMediaWebAnalysis/apiKey.R")
#------------------------------------------------------------------------
library(httr)
library(jsonlite)
# Video ID'lerini belirtin
video_ids <- c("CYwG8LmjmI8", "5Jwtv1rRa-A", "SupKb2CSrlE", "nGMYhO9OD9w&t=1s")
# Başlangıç değerleri
all_comments <- list()
# Her bir video için döngü
for (video_id in video_ids) {
page_token <- ""
video_comments <- c()
# Her bir video için tüm yorumları alana kadar döngü
while (TRUE) {
# API isteğini oluşturun
url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads",
"?part=snippet",
"&videoId=", video_id,
"&maxResults=50",  # Bir istekte 50 yorum alın
"&key=", api_key,
"&pageToken=", page_token)
# API'den yorumları alın
response <- GET(url)
if (http_error(response)) {
stop("API isteği başarısız oldu. Hata kodu: ", http_status(response)$code)
}
# Yanıtı JSON formatına çevirin
comments_json <- content(response, "text")
comments <- fromJSON(comments_json)
# Sadece yorum metinlerini alın ve birleştirin
comment_texts <- sapply(comments$items, function(item) item$snippet$topLevelComment$snippet$textDisplay)
# Tüm yorumları listeye ekleyin
video_comments <- c(video_comments, comment_texts)
# Bir sonraki sayfa için sayfa belirteci alın
page_token <- comments$nextPageToken
# Sonraki sayfa yoksa döngüyü sonlandır
if (is.null(page_token)) {
break
}
}
# Her bir video için tüm yorumları ana listeye ekleyin
all_comments[[video_id]] <- video_comments
}
print("comments")
View(comments)
View(comments)
show(comments)
str(comments)
for (video_id in video_ids) {
page_token <- ""
video_comments <- c()
# Her bir video için tüm yorumları alana kadar döngü
while (TRUE) {
# API isteğini oluşturun
url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads",
"?part=snippet",
"&videoId=", video_id,
"&maxResults=50",  # Bir istekte 50 yorum alın
"&key=", api_key,
"&pageToken=", page_token)
# API'den yorumları alın
response <- GET(url)
if (http_error(response)) {
stop("API isteği başarısız oldu. Hata kodu: ", http_status(response)$code)
}
# Yanıtı JSON formatına çevirin
comments_json <- content(response, "text")
comments <- fromJSON(comments_json)
# Sadece yorum metinlerini alın ve birleştirin
comment_texts <- sapply(comments$items, function(item) item$snippet$topLevelComment$snippet$textDisplay)
# Tüm yorumları listeye ekleyin
video_comments <- c(video_comments, comment_texts)
# Bir sonraki sayfa için sayfa belirteci alın
page_token <- comments$nextPageToken
# Sonraki sayfa yoksa döngüyü sonlandır
if (is.null(page_token)) {
break
}
}
# Her bir video için tüm yorumları ana listeye ekleyin
all_comments[[video_id]] <- video_comments
}
print(comments$items)
# Gerekli kutuphaneleri yukleyelim
install.packages("httr")
library(httr)
library(jsonlite)
source("D:/Code/R/socialMediaWebAnalysis/apiKey.R")
# Videoların ID'lerini belirtin
video_ids <- c("CYwG8LmjmI8", "5Jwtv1rRa-A", "SupKb2CSrlE", "nGMYhO9OD9w&t=1s")
all_comments <- c()
next_page_token <- ""
while (!is.null(next_page_token)) {
url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads",
"?part=snippet",
"&videoId=", video_ids,
"&maxResults=100", # Sayfa baED1na en fazla yorum sayD1sD1
"&pageToken=", next_page_token,
"&key=", api_key)
response <- httr::GET(url)
comments <- httr::content(response, "parsed")
# YorumlarD1 al
comments_data <- comments$items
comments_content <- lapply(comments_data, function(x) x$snippet$topLevelComment$snippet$textDisplay)
# YorumlarD1 birleEtir
all_comments <- c(all_comments, comments_content)
# Bir sonraki sayfa iC'in pageToken'D1 gC<ncelle
next_page_token <- comments$nextPageToken
}
for (video_id in video_ids) {
next_page_token <- ""
video_comments <- c()
while (!is.null(next_page_token)) {
url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads",
"?part=snippet",
"&videoId=", video_id,
"&maxResults=100", # Sayfa başına en fazla yorum sayısı
"&pageToken=", next_page_token,
"&key=", api_key)
response <- httr::GET(url)
comments <- httr::content(response, "parsed")
# Yorumları al
comments_data <- comments$items
comments_content <- lapply(comments_data, function(x) x$snippet$topLevelComment$snippet$textDisplay)
# Yorumları birleştir
video_comments <- c(video_comments, comments_content)
# Bir sonraki sayfa için pageToken'ı güncelle
next_page_token <- comments$nextPageToken
}
# Tüm yorumları all_comments listesine ekleyin
all_comments <- c(all_comments, list(video_comments))
}
writeLines(all_comments, "comments_output3.txt")
# Tüm yorumları birleştirerek bir karakter vektörü oluştur
all_comments_combined <- unlist(all_comments)
# Karakter vektörünü dosyaya yaz
writeLines(all_comments_combined, "comments_output3.txt")
# TC<m yorumlarD1 birleEtirerek bir karakter vektC6rC< oluEtur
all_comments_combined <- unlist(all_comments)
# Karakter vektC6rC<nC< dosyaya yaz
writeLines(all_comments_combined, "comments_output4.txt")
# Karakter vektorunu dosyaya yazdiralim.
writeLines(all_comments_combined, "comments_output3.txt")
# Karakter vektorunu dosyaya yazdiralim.
writeLines(all_comments_combined, "comments_output.txt")
#---------------------------------------------------------------------------#
library(stringi)
#---------------------------------------------------------------------------#
install.packages("stringi")
library(stringi)
View(all_comments)
# Yorumlardaki emojileri kaldırma
comments_content <- lapply(comments_data, function(x) x$snippet$topLevelComment$snippet$textDisplay)
# Karakter vektorunu dosyaya yazdiralim.
writeLines(all_comments_combined, "comments_output1.txt")
# Yorumlardaki emojileri kaldırma
all_comments_combined <- lapply(comments_data, function(x) x$snippet$topLevelComment$snippet$textDisplay)
View(all_comments_combined)
writeLines(all_comments_combined, "comments_output2.txt")
# Tüm yorumları birleştirerek bir karakter vektörü oluşturalım.
all_comments_combined <- unlist(all_comments, use.names = FALSE)
# Listenin içeriğini doğrudan dosyaya yazdıralım.
write(all_comments, "comments_output3.txt")
# Dosyaya yazma işlemi için bir bağlam oluşturalım
output_file <- file("comments_output3.txt", "w")
# Tüm yorumları dosyaya yazdıralım
for (comments in all_comments) {
cat(comments, file = output_file, sep = "\n")
}
# Dosyaya yazma işlemi için bir bağlam oluşturalım
output_file <- file("comments_output3.txt", "w")
# Tüm yorumları dosyaya yazdıralım
for (comments in all_comments) {
for (comment in comments) {
cat(comment, file = output_file, "\n")
}
}
# Dosyaya yazma işlemi için bir bağlam oluşturalım
output_file <- file("comments_output4.txt", "w")
# Tüm yorumları dosyaya yazdıralım
for (comments in all_comments) {
for (comment in comments) {
cat(comment, file = output_file, "\n")
}
}
# Dosya bağlamını kapatalım
close(output_file)
# Tüm yorumları bir veri çerçevesine dönüştürelim
comments_df <- data.frame(comments = unlist(all_comments))
# CSV dosyasına yazdıralım
write.csv(comments_df, "comments_output.csv", row.names = FALSE)
library(httr)
library(jsonlite)
# Api Key'in oldugu dosyanin yolunu belirtiyoruz.
source("D:/Code/R/socialMediaWebAnalysis/apiKey.R")
# Videolarin ID'lerini belirtelim.
video_ids <- c("CYwG8LmjmI8", "5Jwtv1rRa-A", "SupKb2CSrlE", "nGMYhO9OD9w&t=1s")
# Tum videolari bir vektore atayalim.
all_comments <- c()
# YouTube Data v3 Api en fazla 50 yorum cekebiliyor. Her 50 yorum 1 sayfa seklinde tanimlanmis.
#Tum sayfalari bir data frame'e atamak icin bir dongu yaziyorum.
next_page_token <- ""
for (video_id in video_ids) {
next_page_token <- ""
video_comments <- c()
while (!is.null(next_page_token)) {
url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads",
"?part=snippet",
"&videoId=", video_id,
"&maxResults=100", # Sayfa basina en fazla yorum sayisi
"&pageToken=", next_page_token,
"&key=", api_key)
response <- httr::GET(url)
comments <- httr::content(response, "parsed")
# Yorumlari alalim.
comments_data <- comments$items
comments_content <- lapply(comments_data, function(x) x$snippet$topLevelComment$snippet$textDisplay)
# Yorumlari birlestirelim
video_comments <- c(video_comments, comments_content)
# Bir sonraki sayfa icin pageToken'i guncelleyelim.
next_page_token <- comments$nextPageToken
}
# Tum yorumlari all_comments listesine ekleyelim.
all_comments <- c(all_comments, list(video_comments))
}
# Tum yorumlari birlestirerek bir karakter vektoru olusturalim.
all_comments_combined <- unlist(all_comments)
# Karakter vektorunu dosyaya yazdiralim.
writeLines(all_comments_combined, "comments_output.txt")
# Karakter vektorunu dosyaya yazdiralim.
writeLines(all_comments_combined, "comments_output.txt")
setwd("D:/socialMediaWebAnalysis/dataExtractionPhase/comments_output.txt")
source("D:/socialMediaWebAnalysis/dataExtractionPhase/comments_output.txt")
source("D:/socialMediaWebAnalysis/dataExtractionPhase")
source("D:/Code/R/socialMediaWebAnalysis/dataExtractionPhase/comments_output.txtcomments_output.txt")
comments <- readLines("D:/Code/R/socialMediaWebAnalysis/dataExtractionPhase/comments_output.txtcomments_output.txt")
comments <- readLines("D:/Code/R/socialMediaWebAnalysis/dataExtractionPhase/comments_output.txt")
install.packages("tm")
# Data duzenleme adimi
# Cekilmis olan verileri cagırıyoruz.
unorganizedData <- readLines("D:/Code/R/socialMediaWebAnalysis/dataExtractionPhase/comments_output.txt")
# Text duzenleme
library(tm)
# Text duzenleme
library(tm)
# Bosluklari temizleme adimi
unorganizedData <- gsub("\\s+", " ", unorganizedData)
# Data duzenleme adimi
# Cekilmis olan verileri cagırıyoruz.
unorganizedData <- readLines("D:/Code/R/socialMediaWebAnalysis/dataExtractionPhase/comments_output.txt")
# Text duzenlemek icin "tm" paketini yukluyoruz.
library(tm)
# Bosluklari temizleme adimi
cleanedText  <- gsub("\\s+", " ", unorganizedData)
# Buyuk harfleri kucuk harfe cevirme adimi
cleanedText <- tolower(cleanedText)
# Rakamlari kelimeye cevirme adimi
cleanedText <- gsub("\\b\\d+\\b", "number", cleanedText)
# Ozel karakterleri silme adimi
cleanedText <- gsub("[^[:alpha:] ]", "", cleaned_text)
# Ozel karakterleri silme adimi
cleanedText <- gsub("[^[:alpha:] ]", "", cleanedText)
# Temizlenmis metin dosyasini yeni bir metin dosyasina kaydediyoruz.
writeLines(cleanedText, "cleanedDataSet.txt")
setwd("D:/Code/R/socialMediaWebAnalysis/dataEditingPhase")
getwd()
install.packages("tokenizers")
#Tokenize etme adimi
library(tokenizers)
#Function written to tokenize text
tokenized_text <- function(text) {
tokens <- unlist(tokenize_words(text))
return(tokens)
}
tokens <- lapply(text_content, tokenized_text)
file_path <- "cleanedDataSet.txt"
text_content <- readLines(file_path)
#Function written to tokenize text
tokenized_text <- function(text) {
tokens <- unlist(tokenize_words(text))
return(tokens)
}
tokens <- lapply(text_content, tokenized_text)
print(tokens)
install.packages("tidytext")
#Stop words temizliyoruz.
library(tidytext)
# Data Frame olusturalim
dataYtComments <- data.frame(tokens = unlist(tokens))
#Stop words listesi
custom_stop_words <- c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves",
"he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them",
"their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am",
"is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did",
"doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at",
"by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after",
"above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again",
"further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both",
"each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same",
"so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now", "d", "ll", "m",
"o", "re", "ve", "y", "ain", "aren", "couldn", "didn", "doesn", "hadn", "hasn", "haven", "isn", "ma",
"mightn", "mustn", "needn", "shan", "shouldn", "wasn", "weren", "won", "wouldn", "im", "get", "thot", "us")
# Data Frame'de belirtilen Stop word sozcuklerinin temizleme adimi
cleaned_tokens <- dataYtComments %>%
filter(!tokens %in% custom_stop_words)
install.packages("magrittr")
library(magrittr)
# Data Frame'de belirtilen Stop word sozcuklerinin temizleme adimi
cleaned_tokens <- dataYtComments %>%
filter(!tokens %in% custom_stop_words)
print(cleaned_tokens)
View(dataYtComments)
print(cleaned_tokens)
print(head(cleaned_tokens, 10))
install.packages("dplyr")
#Stop words temizliyoruz.
library(dplyr)
# Data Frame olusturalim
dataYtComments <- data.frame(tokens = unlist(tokens))
#Stop words listesi
custom_stop_words <- c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves",
"he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them",
"their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am",
"is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did",
"doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at",
"by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after",
"above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again",
"further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both",
"each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same",
"so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now", "d", "ll", "m",
"o", "re", "ve", "y", "ain", "aren", "couldn", "didn", "doesn", "hadn", "hasn", "haven", "isn", "ma",
"mightn", "mustn", "needn", "shan", "shouldn", "wasn", "weren", "won", "wouldn", "im", "get", "thot", "us")
# Data Frame'de belirtilen Stop word sozcuklerinin temizleme adimi
cleaned_tokens <- dataYtComments %>%
filter(!tokens %in% custom_stop_words)
print(cleaned_tokens)
# Baglaclari temizleme adimi
library(tidytext)
#Conjunction list
conjunctions <- c("and", "or", "but", "nor", "so", "for", "yet", "because", "eventually", "therefore", "finally",
"since", "unless", "if", "after", "while", "then", "during", "until", "hence", "as", "only", "oh",
"wherever", "however", "before", "when", "whenever", "though", "although", "nonetheless", "moreover", "besides", "also", "be")
#Veri cercevesinde belirtilen baglacın temizleme adimi
cleaned_tokens <- cleaned_tokens %>%
filter(!tokens %in% conjunctions)
print(cleaned_tokens)
# Stop words'den ve baglaclardan arindirilmis verileri txt dosyasina yazdirma islemini uyguluyoruz.
cleanedDataYtComments <- cleaned_tokens
# Stop words'den ve baglaclardan arindirilmis verileri txt dosyasina yazdirma islemini uyguluyoruz.
cleaned_data <- cleaned_tokens
output_file <- "D:/Code/R/socialMediaWebAnalysis/dataEditingPhase/cleanedStopWordsAndConjunctions.txt"
writeLines(cleaned_data$tokens, output_file)
